{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (1D example)\n",
    "\n",
    "<span style=\"color:red\">**Please note the HINTS printed in red color**</span>\n",
    "\n",
    "In this example we have only one variable $x$. The parameter vector is $\\boldsymbol\\theta=\\left(\\theta_0,\\theta_1\\right)$.\n",
    "\n",
    "Analogously to the exercises before, the code snippets in this notebook contain several gaps, which should be completed by you. \n",
    "The precise location of the gap is indicated by the expression *MY CODE*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case, random numbers are created by two different gaussian distributions with identical variance and we also know the labels from which distribution each random number was originating from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.88243035 5.03160961 5.88963825 4.48770923 5.22359224 5.1537672\n",
      " 4.70546514 5.59380221 4.10872945 5.48631368 4.16810838 5.25124832\n",
      " 5.82071436 5.75862331 5.71023285 5.582767   4.80479701 5.32970841\n",
      " 3.97822926 5.65146914 5.12006867 4.5067989  4.94598497 4.28024157\n",
      " 3.9586015  3.99445527 6.92637687 4.65200587 4.07250624 5.40658845\n",
      " 4.80703967 5.25258013 4.16493291 3.67706311 4.67240562 4.56092475\n",
      " 5.57555222 4.65701633 4.1571061  4.23272114 5.07080564 5.44024776\n",
      " 6.48789383 4.70660764 6.47501754 4.84916264 5.26406204 5.01393145\n",
      " 4.66570266 3.4954732  5.73004392 5.11778432 4.83241218 4.74386171\n",
      " 4.52275406 5.14261278 5.9480468  5.54747601 7.13211596 4.75845446\n",
      " 5.80289621 4.41228058 5.54515707 3.84461456 4.88517172 5.83727279\n",
      " 5.90996009 4.15005548 5.71848627 5.50414999 3.1497211  5.73510957\n",
      " 4.0815177  6.0980459  3.08288316 5.56759658 4.56950105 4.14715783\n",
      " 4.84096997 4.35139668 5.96180504 5.5895523  4.9044736  4.58961261\n",
      " 5.08204655 5.19900698 4.27436569 5.92605505 5.48374605 5.08014591\n",
      " 5.36760005 4.98433766 5.62150694 4.74859448 3.8732369  4.53290975\n",
      " 4.79001018 5.77062608 4.9343171  4.52427112]\n"
     ]
    }
   ],
   "source": [
    "a=np.random.normal(5,0.7,100)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.37073921 2.64550283 2.68640087 2.54438463 2.00758926 1.50223151\n",
      " 3.04946479 1.03764693 2.59464379 1.40886403 1.2076565  0.8992017\n",
      " 0.96271675 2.75834739 1.82571841 2.39088512 1.49993324 2.23329086\n",
      " 0.74333619 2.34590411 2.0985901  1.70379025 2.27775159 2.19087127\n",
      " 2.01119157 1.94279672 1.9706122  0.94216063 2.37606924 2.01747358\n",
      " 0.83465721 1.62286683 1.5315306  2.50640541 2.08408244 2.37859601\n",
      " 1.21682659 2.56612321 2.57316886 1.54435241 3.58620091 3.01569201\n",
      " 1.49730869 1.77374968 1.31004739 2.08570174 2.75934709 0.77079957\n",
      " 2.60326439 2.7638039  1.10657311 2.73671462 2.50310223 2.80651162\n",
      " 2.352652   1.54664867 2.35003989 1.91291448 1.55080322 2.27827364\n",
      " 1.26566844 2.94292093 2.11080565 2.05224426 1.52271003 3.33870058\n",
      " 2.18130776 1.13624438 1.62848085 1.35081988 1.82353174 1.21168471\n",
      " 1.89162074 2.24190775 1.93464389 1.50297957 1.64404669 1.51854745\n",
      " 2.039633   2.38180259 1.80508852 0.59978743 1.2389929  2.30109105\n",
      " 1.67691244 1.65838034 2.97265678 1.78815894 2.20564    2.07150331\n",
      " 2.36600702 2.87603345 1.22896878 3.71805241 2.02858912 1.90215612\n",
      " 2.75623704 2.19734193 1.41545134 3.06092173]\n"
     ]
    }
   ],
   "source": [
    "b=np.random.normal(2,0.7,100)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot both type of points in a common diagram, where the points generated by the first distribution are plotted in orange at y=1, while the points of the second distribution are plotted in blue at y=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zeros=[0]*100\n",
    "ag=np.column_stack((zeros,a))\n",
    "ones=[1]*100\n",
    "bg=np.column_stack((ones,b))\n",
    "plt.scatter(a,zeros)\n",
    "plt.scatter(b,ones)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to determine, if an arbitrary previously unseen point belongs rather to class $y=0$ or to class $y=1$. For that, we want to employ logistic regression. Similar to linear regression, we firt consider a model with a single independent variable $x$ and two bias parameters $\\theta_0,\\theta_1$.\n",
    "\n",
    "The probability, that $x$ belongs to either of the two classes is determined using the logistic function\n",
    "\n",
    "$$\n",
    "  \\sigma(x) = \\frac{1}{1+\\exp(-x\\theta_1-\\theta_0)}\n",
    "$$\n",
    "\n",
    "<span style=\"color:red\">Hint: use the the function $\\texttt{math.exp}$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-89da463b0e90>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-89da463b0e90>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    return 1.0/(*YOUR_CODE*)\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def logistic_function(x,b0,b1):\n",
    "    return 1.0/(*YOUR_CODE*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For applying gradient descent, we define the two gradients. First the partial derivative with respect to $\\theta_0$ (the first component of the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient0(X,y,theta0,theta1):\n",
    "    num_rows=X.shape[0]\n",
    "    s=0.0\n",
    "    for i in range(0,num_rows):\n",
    "        s=s+(logistic_function(X[i],theta0,theta1)-y[i])\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the partial derivative with respect to $\\theta_1$ (the second component of the gradient)\n",
    "\n",
    "<span style=\"color:red\">Hint: there is only one small difference to $\\texttt{gradient0}$. For Details study Slides on Logistic Regression p23/34 (last line)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient1(X,y,theta0,theta1):\n",
    "    *YOUR CODE* Note that the function gradient1 is not identical with gradient0 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function $J(\\boldsymbol\\theta)$, where $\\boldsymbol\\theta=\\left(\\theta_0,\\theta_1\\right)$ is given by\n",
    "\n",
    "$$\n",
    "    J(\\boldsymbol\\theta) =\n",
    "      - \\frac{1}{n} \\sum_{i=1}^n%\n",
    "        \\left[y_i\\log h(\\boldsymbol\\theta,\\mathbf{X_i})\n",
    "            + (1-y_i)\\log\\left(\n",
    "               1-h(\\boldsymbol\\theta,\\mathbf{X_i})\\right)\\right]\n",
    "$$\n",
    "\n",
    "where $h(\\boldsymbol\\theta,\\mathbf{X_i})=\\sigma\\left(\\mathbf{X_i}^T\\boldsymbol\\theta\\right)=\\sigma\\left(\\theta_0+\\theta_1 x\\right)$ and $\\sigma$ is the sigmoid function.\n",
    "\n",
    "<span style=\"color:red\">Hint: The cost function of Logistic Regression is given on slide p20/34. For the log-function use $\\texttt{math.log}$. Don't forget to divide by $\\texttt{num_rows}$ which is equal to $\\texttt{X.shape[0]}$.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,y,theta0,theta1):\n",
    "    num_rows=X.shape[0]\n",
    "    s=*YOUR CODE *\n",
    "    for i in range(0,*YOUR CODE*):\n",
    "        logit=*YOUR CODE*(X[i],*YOUR CODE*)\n",
    "        s=s+Y[i]*YOUR CODE*\n",
    "    return *YOUR CODE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit our logistic regression model, we combine the vectors a and b in one vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate((a,b))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously, we build our vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.concatenate((zeros,ones))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to determine the optimal values for the parameters $\\theta_0$ and $\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the initial values to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0=1.0\n",
    "theta1=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the learning rate $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,num_iterations):\n",
    "    gr0=gradient0(X,y,theta0,theta1)\n",
    "    gr1=gradient1(X,y,theta0,theta1)\n",
    "    costs=cost_function(X,y,theta0,theta1)\n",
    "    print (\"current costs: \",costs)\n",
    "    print (\"current parameters: \",theta0,\",\",theta1)\n",
    "    theta0=theta0-alpha*gr0\n",
    "    theta1=theta1-alpha*gr1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is given by the x such that: $-\\theta_0-\\theta_1 x=0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this equation for x: $x=-\\frac{\\theta_0}{\\theta_1}$\n",
    "Now let us plot the decision boundary and the logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.arange(-1.0, 7.0, 0.01)\n",
    "fs=[]\n",
    "for t in ts:\n",
    "    s = logistic_function(t,theta0,theta1)\n",
    "    fs.append(s)\n",
    "    \n",
    "# plot the decision boundary\n",
    "plt.axvline(*YOUR CODE*)\n",
    "\n",
    "# plot the logistic function with the parameters determined by gradient descent\n",
    "plt.plot(ts, fs)  \n",
    "plt.scatter(a,zeros)\n",
    "plt.scatter(b,ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Real World Example)\n",
    "\n",
    "Now we want to conduct a logistic regression with multiple independent variables.  For that, we load a dataset for skin eczemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"skin_disease.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remap the class values where 255 is mapped to 1 (eczema) and 1 to 0 (no eczema)\n",
    "Since the old and new value range overlap (1 occurs in both). We have to remap the values in two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"class\"]==255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {1: 0, 255: 2}\n",
    "df=df.replace({\"class\": di})\n",
    "df[df[\"class\"]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {2: 1}\n",
    "df=df.replace({\"class\": di})\n",
    "df[df[\"class\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the coordinates x,y  and the fields expertsEczemaVotePatch, expertsEczemaAllVotePatch and  expertsEczemaVoteCenterPixel from the dataset. The last three attributes are all derived from the expert labelling and must therefore not be used as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del df[\"x\"]\n",
    "del df[\"y\"]\n",
    "del df[\"expertsEczemaVotePatch\"]\n",
    "del df[\"expertsEczemaAllVotePatch\"]\n",
    "del df[\"expertsEczemaVoteCenterPixel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we split the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent and dependent variables are separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train.drop('class', axis=1)\n",
    "X_test = test.drop('class', axis=1)\n",
    "\n",
    "y_train = train[\"class\"]\n",
    "y_test = test[\"class\"]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create the logistic regression object by instantiate the appropriate sklearn class\n",
    "\n",
    "<span style=\"color:red\">Hint: use $\\texttt{help(sklearn.linear_model)}$; You can also use google with \"sklearn\" \"logistic\" \"regression\" \"example\" (all 4 words without quotes and then find https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg=*YOUR CODE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and fit the model to the variables <span style=\"color:red\">using something like $\\texttt{logReg.fit(...,...)}$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*YOUR CODE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the model to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=logReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evaluation metrics we use accuracy and F1-Score <span style=\"color:red\">using something like $\\texttt{sklearn.metrics.accuracy_score(...,...)}$ and $\\texttt{sklearn.metrics.f1_score(...,...)}$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=*YOUR CODE*\n",
    "fscore=*YOUR CODE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both metrics should be printed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"accuracy: \",accuracy)\n",
    "print (\"f1 score: \",fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
